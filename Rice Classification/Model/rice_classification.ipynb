{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bIBywUDZgjGu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d muratkokludataset/rice-image-dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urDuQ5Gzh0oM",
        "outputId": "17266302-cbc4-48df-f087-b9b2c3e650ee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/muratkokludataset/rice-image-dataset\n",
            "License(s): CC0-1.0\n",
            "rice-image-dataset.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "def extract_zip_to_directory(zip_path, extract_dir):\n",
        "    \"\"\"\n",
        "    Extracts the contents of a ZIP file to a specified directory.\n",
        "\n",
        "    Parameters:\n",
        "    zip_path (str): Path to the ZIP file.\n",
        "    extract_dir (str): Directory where the contents should be extracted.\n",
        "    \"\"\"\n",
        "    # Create the target directory if it doesn't exist\n",
        "    os.makedirs(extract_dir, exist_ok=True)\n",
        "\n",
        "    # Open the ZIP file\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # Extract all contents into the target directory\n",
        "        zip_ref.extractall(extract_dir)\n",
        "\n",
        "# Example usage\n",
        "zip_path = '/content/rice-image-dataset.zip'  # Path to your ZIP file\n",
        "extract_dir = '/content/sample_data'  # Directory where you want to extract the files\n",
        "\n",
        "extract_zip_to_directory(zip_path, extract_dir)\n"
      ],
      "metadata": {
        "id": "A13WO-L_iP58"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import Dataset, DataLoader, ConcatDataset, random_split\n",
        "from PIL import Image\n",
        "\n",
        "resize_transforms = transforms.Compose([transforms.Resize((128,128)), transforms.ToTensor()])\n",
        "\n",
        "in_folders = ['./Rice/Ipsala/', './Rice/Arborio/', './Rice/Basmati/', './Rice/Jasmine/','./Rice/Karacadag/' ]\n",
        "\n",
        "# Path to the root directory containing subdirectories for each class\n",
        "root_dir = './sample_data/Rice_Image_Dataset'\n",
        "\n",
        "# Create the dataset using ImageFolder\n",
        "dataset = datasets.ImageFolder(root=root_dir, transform=resize_transforms)\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 256\n",
        "\n",
        "# Determine the sizes of the training and test datasets\n",
        "total_size = len(dataset)\n",
        "test_size = int(0.2 * total_size)  # 20% for testing\n",
        "train_size = total_size - test_size  # Remaining 80% for training\n",
        "\n",
        "# Split the dataset into training and test datasets\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "# Create DataLoaders for training and test datasets\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "id": "dflWInwxi_jV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class SimpleCNN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=4, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(4)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=4, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(16)\n",
        "\n",
        "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
        "\n",
        "        self.fc1 = nn.Linear(16 * 32 * 32, 15)  # Adjust according to the image size\n",
        "        self.bn3 = nn.BatchNorm1d(15)\n",
        "        self.fc2 = nn.Linear(15, 5)          # Number of output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.tanh(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(torch.tanh(self.bn2(self.conv2(x))))\n",
        "\n",
        "        x = x.view(-1, 16 * 32 * 32)  # Flatten the tensor\n",
        "\n",
        "        x = torch.tanh((self.fc1(x)))\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "WKAh9n9KjlR_"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# Hyper Parameters\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model = SimpleCNN()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.95)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "\n",
        "epochs = 2\n",
        "total_samples = len(train_dataset)\n",
        "n_iterations = math.ceil(total_samples/256)\n",
        "\n",
        "# Training\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    model.train()\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        print(f'epoch {epoch+1}/{epochs}, step {i+1} / {n_iterations} ')\n",
        "\n",
        "        outputs = model.forward(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        print(f'loss = {loss}')\n",
        "        train_losses.append(loss.item())\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            avg_val_loss = 0\n",
        "            for inputs, labels in test_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                print(f'Validation Loss = {loss.item()}')\n",
        "                val_losses.append(loss.item())\n",
        "                break\n",
        "\n",
        "\n",
        "# Assume `model` is your trained PyTorch model\n",
        "# Save the model's state dictionary\n",
        "torch.save(model.state_dict(), 'model_weights.pth')\n",
        "\n",
        "\n",
        "# nv1 = nn.Conv2d(in_channels = 3, out_channels= 16, kernel_size=3, stride=1, padding=1)\n",
        "# nv2 = nn.Conv2d(in_channels = 16, out_channels= 32, kernel_size=3, stride=1, padding=1)\n",
        "# nv3 = nn.Conv2d(in_channels = 32, out_channels= 64, kernel_size=3, stride=1, padding=1)\n",
        "#\n",
        "# pool = nn.MaxPool2d(2, 2)\n",
        "# for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
        "#     print(f'Batch {batch_idx + 1}:')\n",
        "#     print(f'Inputs shape: {inputs.shape}')\n",
        "#     print(f'Labels shape: {labels.shape}')\n",
        "#     x = conv1(inputs)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     x = pool(x)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     x = conv2(x)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     x = pool(x)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     x = conv3(x)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     x = pool(x)\n",
        "#     print(f'x_shape = {x.shape}')\n",
        "#     # Process the batch\n",
        "#     break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4chD92aj5yk",
        "outputId": "b7210ca4-20dd-45cb-cde1-cbd660ab5e60"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 1/2, step 1 / 235 \n",
            "loss = 1.7911330461502075\n",
            "Validation Loss = 1.5586812496185303\n",
            "epoch 1/2, step 2 / 235 \n",
            "loss = 1.5612422227859497\n",
            "Validation Loss = 1.5350086688995361\n",
            "epoch 1/2, step 3 / 235 \n",
            "loss = 1.5291118621826172\n",
            "Validation Loss = 1.5087954998016357\n",
            "epoch 1/2, step 4 / 235 \n",
            "loss = 1.4872766733169556\n",
            "Validation Loss = 1.4662855863571167\n",
            "epoch 1/2, step 5 / 235 \n",
            "loss = 1.469583511352539\n",
            "Validation Loss = 1.4349421262741089\n",
            "epoch 1/2, step 6 / 235 \n",
            "loss = 1.4524600505828857\n",
            "Validation Loss = 1.352931022644043\n",
            "epoch 1/2, step 7 / 235 \n",
            "loss = 1.3600153923034668\n",
            "Validation Loss = 1.3003251552581787\n",
            "epoch 1/2, step 8 / 235 \n",
            "loss = 1.2905683517456055\n",
            "Validation Loss = 1.2673423290252686\n",
            "epoch 1/2, step 9 / 235 \n",
            "loss = 1.2783738374710083\n",
            "Validation Loss = 1.2263132333755493\n",
            "epoch 1/2, step 10 / 235 \n",
            "loss = 1.2268791198730469\n",
            "Validation Loss = 1.1611502170562744\n",
            "epoch 1/2, step 11 / 235 \n",
            "loss = 1.1581664085388184\n",
            "Validation Loss = 1.0994696617126465\n",
            "epoch 1/2, step 12 / 235 \n",
            "loss = 1.1007686853408813\n",
            "Validation Loss = 1.0595154762268066\n",
            "epoch 1/2, step 13 / 235 \n",
            "loss = 1.033226728439331\n",
            "Validation Loss = 0.984133780002594\n",
            "epoch 1/2, step 14 / 235 \n",
            "loss = 1.0055054426193237\n",
            "Validation Loss = 0.9490098357200623\n",
            "epoch 1/2, step 15 / 235 \n",
            "loss = 0.955985963344574\n",
            "Validation Loss = 0.9018481373786926\n",
            "epoch 1/2, step 16 / 235 \n",
            "loss = 0.9299915432929993\n",
            "Validation Loss = 0.8812344670295715\n",
            "epoch 1/2, step 17 / 235 \n",
            "loss = 0.8725832104682922\n",
            "Validation Loss = 0.80974280834198\n",
            "epoch 1/2, step 18 / 235 \n",
            "loss = 0.8410104513168335\n",
            "Validation Loss = 0.7850935459136963\n",
            "epoch 1/2, step 19 / 235 \n",
            "loss = 0.8161572813987732\n",
            "Validation Loss = 0.7885773777961731\n",
            "epoch 1/2, step 20 / 235 \n",
            "loss = 0.7615243792533875\n",
            "Validation Loss = 0.7283602952957153\n",
            "epoch 1/2, step 21 / 235 \n",
            "loss = 0.7354034185409546\n",
            "Validation Loss = 0.7046434879302979\n",
            "epoch 1/2, step 22 / 235 \n",
            "loss = 0.6902574300765991\n",
            "Validation Loss = 0.6560194492340088\n",
            "epoch 1/2, step 23 / 235 \n",
            "loss = 0.6439830660820007\n",
            "Validation Loss = 0.6418771743774414\n",
            "epoch 1/2, step 24 / 235 \n",
            "loss = 0.6553112864494324\n",
            "Validation Loss = 0.6234545707702637\n",
            "epoch 1/2, step 25 / 235 \n",
            "loss = 0.6252737641334534\n",
            "Validation Loss = 0.5997695326805115\n",
            "epoch 1/2, step 26 / 235 \n",
            "loss = 0.5947327017784119\n",
            "Validation Loss = 0.563302218914032\n",
            "epoch 1/2, step 27 / 235 \n",
            "loss = 0.5595971345901489\n",
            "Validation Loss = 0.5632054805755615\n",
            "epoch 1/2, step 28 / 235 \n",
            "loss = 0.5195988416671753\n",
            "Validation Loss = 0.4956008195877075\n",
            "epoch 1/2, step 29 / 235 \n",
            "loss = 0.47625932097435\n",
            "Validation Loss = 0.5117482542991638\n",
            "epoch 1/2, step 30 / 235 \n",
            "loss = 0.49223247170448303\n",
            "Validation Loss = 0.48229897022247314\n",
            "epoch 1/2, step 31 / 235 \n",
            "loss = 0.4593006372451782\n",
            "Validation Loss = 0.4527862071990967\n",
            "epoch 1/2, step 32 / 235 \n",
            "loss = 0.4279317557811737\n",
            "Validation Loss = 0.45372167229652405\n",
            "epoch 1/2, step 33 / 235 \n",
            "loss = 0.46881213784217834\n",
            "Validation Loss = 0.3979369103908539\n",
            "epoch 1/2, step 34 / 235 \n",
            "loss = 0.40151071548461914\n",
            "Validation Loss = 0.3930248022079468\n",
            "epoch 1/2, step 35 / 235 \n",
            "loss = 0.42575937509536743\n",
            "Validation Loss = 0.42466381192207336\n",
            "epoch 1/2, step 36 / 235 \n",
            "loss = 0.4335084557533264\n",
            "Validation Loss = 0.37999919056892395\n",
            "epoch 1/2, step 37 / 235 \n",
            "loss = 0.3469696342945099\n",
            "Validation Loss = 0.3619789183139801\n",
            "epoch 1/2, step 38 / 235 \n",
            "loss = 0.40419381856918335\n",
            "Validation Loss = 0.37526097893714905\n",
            "epoch 1/2, step 39 / 235 \n",
            "loss = 0.346122682094574\n",
            "Validation Loss = 0.3443596065044403\n",
            "epoch 1/2, step 40 / 235 \n",
            "loss = 0.33183372020721436\n",
            "Validation Loss = 0.3278496265411377\n",
            "epoch 1/2, step 41 / 235 \n",
            "loss = 0.3437100946903229\n",
            "Validation Loss = 0.34286704659461975\n",
            "epoch 1/2, step 42 / 235 \n",
            "loss = 0.36732640862464905\n",
            "Validation Loss = 0.32764768600463867\n",
            "epoch 1/2, step 43 / 235 \n",
            "loss = 0.2710598111152649\n",
            "Validation Loss = 0.3211784064769745\n",
            "epoch 1/2, step 44 / 235 \n",
            "loss = 0.27919620275497437\n",
            "Validation Loss = 0.3177057206630707\n",
            "epoch 1/2, step 45 / 235 \n",
            "loss = 0.2939208447933197\n",
            "Validation Loss = 0.2741564214229584\n",
            "epoch 1/2, step 46 / 235 \n",
            "loss = 0.2709917426109314\n",
            "Validation Loss = 0.25752031803131104\n",
            "epoch 1/2, step 47 / 235 \n",
            "loss = 0.2876080572605133\n",
            "Validation Loss = 0.22910523414611816\n",
            "epoch 1/2, step 48 / 235 \n",
            "loss = 0.2580797076225281\n",
            "Validation Loss = 0.239951491355896\n",
            "epoch 1/2, step 49 / 235 \n",
            "loss = 0.2529725134372711\n",
            "Validation Loss = 0.21059201657772064\n",
            "epoch 1/2, step 50 / 235 \n",
            "loss = 0.2230917513370514\n",
            "Validation Loss = 0.24966011941432953\n",
            "epoch 1/2, step 51 / 235 \n",
            "loss = 0.22174590826034546\n",
            "Validation Loss = 0.23447121679782867\n",
            "epoch 1/2, step 52 / 235 \n",
            "loss = 0.22364138066768646\n",
            "Validation Loss = 0.22864659130573273\n",
            "epoch 1/2, step 53 / 235 \n",
            "loss = 0.21762041747570038\n",
            "Validation Loss = 0.27487245202064514\n",
            "epoch 1/2, step 54 / 235 \n",
            "loss = 0.22422541677951813\n",
            "Validation Loss = 0.2254813015460968\n",
            "epoch 1/2, step 55 / 235 \n",
            "loss = 0.19455239176750183\n",
            "Validation Loss = 0.2780010402202606\n",
            "epoch 1/2, step 56 / 235 \n",
            "loss = 0.22605732083320618\n",
            "Validation Loss = 0.23465511202812195\n",
            "epoch 1/2, step 57 / 235 \n",
            "loss = 0.21311748027801514\n",
            "Validation Loss = 0.30076441168785095\n",
            "epoch 1/2, step 58 / 235 \n",
            "loss = 0.22230572998523712\n",
            "Validation Loss = 0.21322943270206451\n",
            "epoch 1/2, step 59 / 235 \n",
            "loss = 0.1893075406551361\n",
            "Validation Loss = 0.20531946420669556\n",
            "epoch 1/2, step 60 / 235 \n",
            "loss = 0.19626648724079132\n",
            "Validation Loss = 0.2487906813621521\n",
            "epoch 1/2, step 61 / 235 \n",
            "loss = 0.228677436709404\n",
            "Validation Loss = 0.22509954869747162\n",
            "epoch 1/2, step 62 / 235 \n",
            "loss = 0.2154274880886078\n",
            "Validation Loss = 0.19919367134571075\n",
            "epoch 1/2, step 63 / 235 \n",
            "loss = 0.1853281557559967\n",
            "Validation Loss = 0.18900907039642334\n",
            "epoch 1/2, step 64 / 235 \n",
            "loss = 0.22076264023780823\n",
            "Validation Loss = 0.23368391394615173\n",
            "epoch 1/2, step 65 / 235 \n",
            "loss = 0.22715170681476593\n",
            "Validation Loss = 0.29419124126434326\n",
            "epoch 1/2, step 66 / 235 \n",
            "loss = 0.24844688177108765\n",
            "Validation Loss = 0.24937494099140167\n",
            "epoch 1/2, step 67 / 235 \n",
            "loss = 0.23928329348564148\n",
            "Validation Loss = 0.18806864321231842\n",
            "epoch 1/2, step 68 / 235 \n",
            "loss = 0.1747722625732422\n",
            "Validation Loss = 0.31194519996643066\n",
            "epoch 1/2, step 69 / 235 \n",
            "loss = 0.3051829934120178\n",
            "Validation Loss = 0.1620825231075287\n",
            "epoch 1/2, step 70 / 235 \n",
            "loss = 0.18440860509872437\n",
            "Validation Loss = 0.3284463584423065\n",
            "epoch 1/2, step 71 / 235 \n",
            "loss = 0.26936590671539307\n",
            "Validation Loss = 0.1795857697725296\n",
            "epoch 1/2, step 72 / 235 \n",
            "loss = 0.13921700417995453\n",
            "Validation Loss = 0.19264769554138184\n",
            "epoch 1/2, step 73 / 235 \n",
            "loss = 0.23299816250801086\n",
            "Validation Loss = 0.14155998826026917\n",
            "epoch 1/2, step 74 / 235 \n",
            "loss = 0.23026694357395172\n",
            "Validation Loss = 0.21736355125904083\n",
            "epoch 1/2, step 75 / 235 \n",
            "loss = 0.150057852268219\n",
            "Validation Loss = 0.2049601525068283\n",
            "epoch 1/2, step 76 / 235 \n",
            "loss = 0.17768453061580658\n",
            "Validation Loss = 0.1565731018781662\n",
            "epoch 1/2, step 77 / 235 \n",
            "loss = 0.13758546113967896\n",
            "Validation Loss = 0.250030517578125\n",
            "epoch 1/2, step 78 / 235 \n",
            "loss = 0.2401479035615921\n",
            "Validation Loss = 0.29038822650909424\n",
            "epoch 1/2, step 79 / 235 \n",
            "loss = 0.16951027512550354\n",
            "Validation Loss = 0.21447373926639557\n",
            "epoch 1/2, step 80 / 235 \n",
            "loss = 0.19043634831905365\n",
            "Validation Loss = 0.2537670433521271\n",
            "epoch 1/2, step 81 / 235 \n",
            "loss = 0.1810634434223175\n",
            "Validation Loss = 0.18132252991199493\n",
            "epoch 1/2, step 82 / 235 \n",
            "loss = 0.20827187597751617\n",
            "Validation Loss = 0.1454290747642517\n",
            "epoch 1/2, step 83 / 235 \n",
            "loss = 0.1584930121898651\n",
            "Validation Loss = 0.192267507314682\n",
            "epoch 1/2, step 84 / 235 \n",
            "loss = 0.1535274088382721\n",
            "Validation Loss = 0.1901976317167282\n",
            "epoch 1/2, step 85 / 235 \n",
            "loss = 0.1745585799217224\n",
            "Validation Loss = 0.1844046711921692\n",
            "epoch 1/2, step 86 / 235 \n",
            "loss = 0.15514160692691803\n",
            "Validation Loss = 0.18909800052642822\n",
            "epoch 1/2, step 87 / 235 \n",
            "loss = 0.1287693977355957\n",
            "Validation Loss = 0.14192496240139008\n",
            "epoch 1/2, step 88 / 235 \n",
            "loss = 0.12506987154483795\n",
            "Validation Loss = 0.13811127841472626\n",
            "epoch 1/2, step 89 / 235 \n",
            "loss = 0.1272042691707611\n",
            "Validation Loss = 0.14039351046085358\n",
            "epoch 1/2, step 90 / 235 \n",
            "loss = 0.18512333929538727\n",
            "Validation Loss = 0.16416428983211517\n",
            "epoch 1/2, step 91 / 235 \n",
            "loss = 0.136490598320961\n",
            "Validation Loss = 0.11535371840000153\n",
            "epoch 1/2, step 92 / 235 \n",
            "loss = 0.16926071047782898\n",
            "Validation Loss = 0.14749427139759064\n",
            "epoch 1/2, step 93 / 235 \n",
            "loss = 0.13130952417850494\n",
            "Validation Loss = 0.181644007563591\n",
            "epoch 1/2, step 94 / 235 \n",
            "loss = 0.1638353019952774\n",
            "Validation Loss = 0.15882070362567902\n",
            "epoch 1/2, step 95 / 235 \n",
            "loss = 0.10027682036161423\n",
            "Validation Loss = 0.1368771642446518\n",
            "epoch 1/2, step 96 / 235 \n",
            "loss = 0.09729786217212677\n",
            "Validation Loss = 0.1403914839029312\n",
            "epoch 1/2, step 97 / 235 \n",
            "loss = 0.14654521644115448\n",
            "Validation Loss = 0.1977245807647705\n",
            "epoch 1/2, step 98 / 235 \n",
            "loss = 0.11357269436120987\n",
            "Validation Loss = 0.1494293063879013\n",
            "epoch 1/2, step 99 / 235 \n",
            "loss = 0.13726620376110077\n",
            "Validation Loss = 0.10353848338127136\n",
            "epoch 1/2, step 100 / 235 \n",
            "loss = 0.10084761679172516\n",
            "Validation Loss = 0.15144646167755127\n",
            "epoch 1/2, step 101 / 235 \n",
            "loss = 0.1354808211326599\n",
            "Validation Loss = 0.11851293593645096\n",
            "epoch 1/2, step 102 / 235 \n",
            "loss = 0.13030920922756195\n",
            "Validation Loss = 0.13007840514183044\n",
            "epoch 1/2, step 103 / 235 \n",
            "loss = 0.17326034605503082\n",
            "Validation Loss = 0.15727366507053375\n",
            "epoch 1/2, step 104 / 235 \n",
            "loss = 0.15285171568393707\n",
            "Validation Loss = 0.16874296963214874\n",
            "epoch 1/2, step 105 / 235 \n",
            "loss = 0.1706940084695816\n",
            "Validation Loss = 0.14231176674365997\n",
            "epoch 1/2, step 106 / 235 \n",
            "loss = 0.10353803634643555\n",
            "Validation Loss = 0.11931842565536499\n",
            "epoch 1/2, step 107 / 235 \n",
            "loss = 0.13329830765724182\n",
            "Validation Loss = 0.176206573843956\n",
            "epoch 1/2, step 108 / 235 \n",
            "loss = 0.13452494144439697\n",
            "Validation Loss = 0.08445950597524643\n",
            "epoch 1/2, step 109 / 235 \n",
            "loss = 0.13343487679958344\n",
            "Validation Loss = 0.11985470354557037\n",
            "epoch 1/2, step 110 / 235 \n",
            "loss = 0.15256386995315552\n",
            "Validation Loss = 0.22833392024040222\n",
            "epoch 1/2, step 111 / 235 \n",
            "loss = 0.1572275012731552\n",
            "Validation Loss = 0.11221253871917725\n",
            "epoch 1/2, step 112 / 235 \n",
            "loss = 0.13022588193416595\n",
            "Validation Loss = 0.1398555040359497\n",
            "epoch 1/2, step 113 / 235 \n",
            "loss = 0.11594364047050476\n",
            "Validation Loss = 0.12872619926929474\n",
            "epoch 1/2, step 114 / 235 \n",
            "loss = 0.14053720235824585\n",
            "Validation Loss = 0.12455732375383377\n",
            "epoch 1/2, step 115 / 235 \n",
            "loss = 0.1738186478614807\n",
            "Validation Loss = 0.09896855056285858\n",
            "epoch 1/2, step 116 / 235 \n",
            "loss = 0.14417198300361633\n",
            "Validation Loss = 0.18382741510868073\n",
            "epoch 1/2, step 117 / 235 \n",
            "loss = 0.12243077903985977\n",
            "Validation Loss = 0.14851891994476318\n",
            "epoch 1/2, step 118 / 235 \n",
            "loss = 0.11920668929815292\n",
            "Validation Loss = 0.1455167979001999\n",
            "epoch 1/2, step 119 / 235 \n",
            "loss = 0.12840276956558228\n",
            "Validation Loss = 0.11837794631719589\n",
            "epoch 1/2, step 120 / 235 \n",
            "loss = 0.1472131311893463\n",
            "Validation Loss = 0.13732382655143738\n",
            "epoch 1/2, step 121 / 235 \n",
            "loss = 0.1123272255063057\n",
            "Validation Loss = 0.14598551392555237\n",
            "epoch 1/2, step 122 / 235 \n",
            "loss = 0.15021292865276337\n",
            "Validation Loss = 0.13988818228244781\n",
            "epoch 1/2, step 123 / 235 \n",
            "loss = 0.1874101608991623\n",
            "Validation Loss = 0.10370076447725296\n",
            "epoch 1/2, step 124 / 235 \n",
            "loss = 0.11746562272310257\n",
            "Validation Loss = 0.1405716836452484\n",
            "epoch 1/2, step 125 / 235 \n",
            "loss = 0.20349538326263428\n",
            "Validation Loss = 0.1888233870267868\n",
            "epoch 1/2, step 126 / 235 \n",
            "loss = 0.23191343247890472\n",
            "Validation Loss = 0.13369204103946686\n",
            "epoch 1/2, step 127 / 235 \n",
            "loss = 0.1282077133655548\n",
            "Validation Loss = 0.13913291692733765\n",
            "epoch 1/2, step 128 / 235 \n",
            "loss = 0.13427452743053436\n",
            "Validation Loss = 0.19609887897968292\n",
            "epoch 1/2, step 129 / 235 \n",
            "loss = 0.14405278861522675\n",
            "Validation Loss = 0.17556177079677582\n",
            "epoch 1/2, step 130 / 235 \n",
            "loss = 0.2177005112171173\n",
            "Validation Loss = 0.13839761912822723\n",
            "epoch 1/2, step 131 / 235 \n",
            "loss = 0.12443824857473373\n",
            "Validation Loss = 0.14072401821613312\n",
            "epoch 1/2, step 132 / 235 \n",
            "loss = 0.1296708583831787\n",
            "Validation Loss = 0.23198892176151276\n",
            "epoch 1/2, step 133 / 235 \n",
            "loss = 0.16347335278987885\n",
            "Validation Loss = 0.2529377341270447\n",
            "epoch 1/2, step 134 / 235 \n",
            "loss = 0.284814715385437\n",
            "Validation Loss = 0.13541892170906067\n",
            "epoch 1/2, step 135 / 235 \n",
            "loss = 0.225080206990242\n",
            "Validation Loss = 0.10305692255496979\n",
            "epoch 1/2, step 136 / 235 \n",
            "loss = 0.11250505596399307\n",
            "Validation Loss = 0.2020903378725052\n",
            "epoch 1/2, step 137 / 235 \n",
            "loss = 0.14924664795398712\n",
            "Validation Loss = 0.21941380202770233\n",
            "epoch 1/2, step 138 / 235 \n",
            "loss = 0.16530030965805054\n",
            "Validation Loss = 0.1886836141347885\n",
            "epoch 1/2, step 139 / 235 \n",
            "loss = 0.15854325890541077\n",
            "Validation Loss = 0.1241656094789505\n",
            "epoch 1/2, step 140 / 235 \n",
            "loss = 0.13385358452796936\n",
            "Validation Loss = 0.1871471405029297\n",
            "epoch 1/2, step 141 / 235 \n",
            "loss = 0.19106879830360413\n",
            "Validation Loss = 0.2695411741733551\n",
            "epoch 1/2, step 142 / 235 \n",
            "loss = 0.2679612636566162\n",
            "Validation Loss = 0.18088476359844208\n",
            "epoch 1/2, step 143 / 235 \n",
            "loss = 0.1112096980214119\n",
            "Validation Loss = 0.1540852040052414\n",
            "epoch 1/2, step 144 / 235 \n",
            "loss = 0.131978377699852\n",
            "Validation Loss = 0.195698544383049\n",
            "epoch 1/2, step 145 / 235 \n",
            "loss = 0.2181844264268875\n",
            "Validation Loss = 0.1411648690700531\n",
            "epoch 1/2, step 146 / 235 \n",
            "loss = 0.19771547615528107\n",
            "Validation Loss = 0.147727370262146\n",
            "epoch 1/2, step 147 / 235 \n",
            "loss = 0.15949709713459015\n",
            "Validation Loss = 0.21859577298164368\n",
            "epoch 1/2, step 148 / 235 \n",
            "loss = 0.2036277949810028\n",
            "Validation Loss = 0.1833345890045166\n",
            "epoch 1/2, step 149 / 235 \n",
            "loss = 0.1527174711227417\n",
            "Validation Loss = 0.11933688819408417\n",
            "epoch 1/2, step 150 / 235 \n",
            "loss = 0.15036273002624512\n",
            "Validation Loss = 0.13418394327163696\n",
            "epoch 1/2, step 151 / 235 \n",
            "loss = 0.10249747335910797\n",
            "Validation Loss = 0.290051132440567\n",
            "epoch 1/2, step 152 / 235 \n",
            "loss = 0.24759541451931\n",
            "Validation Loss = 0.21296536922454834\n",
            "epoch 1/2, step 153 / 235 \n",
            "loss = 0.16628418862819672\n",
            "Validation Loss = 0.13363578915596008\n",
            "epoch 1/2, step 154 / 235 \n",
            "loss = 0.12625090777873993\n",
            "Validation Loss = 0.12726344168186188\n",
            "epoch 1/2, step 155 / 235 \n",
            "loss = 0.1278962939977646\n",
            "Validation Loss = 0.17970098555088043\n",
            "epoch 1/2, step 156 / 235 \n",
            "loss = 0.21379417181015015\n",
            "Validation Loss = 0.1514812409877777\n",
            "epoch 1/2, step 157 / 235 \n",
            "loss = 0.1706356257200241\n",
            "Validation Loss = 0.11722804605960846\n",
            "epoch 1/2, step 158 / 235 \n",
            "loss = 0.16120028495788574\n",
            "Validation Loss = 0.17040814459323883\n",
            "epoch 1/2, step 159 / 235 \n",
            "loss = 0.15551720559597015\n",
            "Validation Loss = 0.18536970019340515\n",
            "epoch 1/2, step 160 / 235 \n",
            "loss = 0.19741912186145782\n",
            "Validation Loss = 0.11655493825674057\n",
            "epoch 1/2, step 161 / 235 \n",
            "loss = 0.12367316335439682\n",
            "Validation Loss = 0.1337675154209137\n",
            "epoch 1/2, step 162 / 235 \n",
            "loss = 0.11958904564380646\n",
            "Validation Loss = 0.13950537145137787\n",
            "epoch 1/2, step 163 / 235 \n",
            "loss = 0.1253831386566162\n",
            "Validation Loss = 0.14881466329097748\n",
            "epoch 1/2, step 164 / 235 \n",
            "loss = 0.0980800986289978\n",
            "Validation Loss = 0.09062055498361588\n",
            "epoch 1/2, step 165 / 235 \n",
            "loss = 0.08428052812814713\n",
            "Validation Loss = 0.12255598604679108\n",
            "epoch 1/2, step 166 / 235 \n",
            "loss = 0.14737774431705475\n",
            "Validation Loss = 0.16047944128513336\n",
            "epoch 1/2, step 167 / 235 \n",
            "loss = 0.086164191365242\n",
            "Validation Loss = 0.1295032501220703\n",
            "epoch 1/2, step 168 / 235 \n",
            "loss = 0.13274125754833221\n",
            "Validation Loss = 0.152188241481781\n",
            "epoch 1/2, step 169 / 235 \n",
            "loss = 0.11205515265464783\n",
            "Validation Loss = 0.12831416726112366\n",
            "epoch 1/2, step 170 / 235 \n",
            "loss = 0.0979960560798645\n",
            "Validation Loss = 0.1565830409526825\n",
            "epoch 1/2, step 171 / 235 \n",
            "loss = 0.09581810981035233\n",
            "Validation Loss = 0.15434229373931885\n",
            "epoch 1/2, step 172 / 235 \n",
            "loss = 0.1248437836766243\n",
            "Validation Loss = 0.11759467422962189\n",
            "epoch 1/2, step 173 / 235 \n",
            "loss = 0.1267600804567337\n",
            "Validation Loss = 0.11531884223222733\n",
            "epoch 1/2, step 174 / 235 \n",
            "loss = 0.12121719866991043\n",
            "Validation Loss = 0.14855815470218658\n",
            "epoch 1/2, step 175 / 235 \n",
            "loss = 0.13493399322032928\n",
            "Validation Loss = 0.16927321255207062\n",
            "epoch 1/2, step 176 / 235 \n",
            "loss = 0.19633153080940247\n",
            "Validation Loss = 0.1585472822189331\n",
            "epoch 1/2, step 177 / 235 \n",
            "loss = 0.16491232812404633\n",
            "Validation Loss = 0.11931554973125458\n",
            "epoch 1/2, step 178 / 235 \n",
            "loss = 0.12875032424926758\n",
            "Validation Loss = 0.10487161576747894\n",
            "epoch 1/2, step 179 / 235 \n",
            "loss = 0.1715538501739502\n",
            "Validation Loss = 0.156927689909935\n",
            "epoch 1/2, step 180 / 235 \n",
            "loss = 0.08110785484313965\n",
            "Validation Loss = 0.17168453335762024\n",
            "epoch 1/2, step 181 / 235 \n",
            "loss = 0.20998619496822357\n",
            "Validation Loss = 0.1388149857521057\n",
            "epoch 1/2, step 182 / 235 \n",
            "loss = 0.17950811982154846\n",
            "Validation Loss = 0.14104101061820984\n",
            "epoch 1/2, step 183 / 235 \n",
            "loss = 0.09220486879348755\n",
            "Validation Loss = 0.11296869069337845\n",
            "epoch 1/2, step 184 / 235 \n",
            "loss = 0.12832164764404297\n",
            "Validation Loss = 0.0933414101600647\n",
            "epoch 1/2, step 185 / 235 \n",
            "loss = 0.13443835079669952\n",
            "Validation Loss = 0.14021943509578705\n",
            "epoch 1/2, step 186 / 235 \n",
            "loss = 0.09830092638731003\n",
            "Validation Loss = 0.12858466804027557\n",
            "epoch 1/2, step 187 / 235 \n",
            "loss = 0.09607404470443726\n",
            "Validation Loss = 0.12669602036476135\n",
            "epoch 1/2, step 188 / 235 \n",
            "loss = 0.15939536690711975\n",
            "Validation Loss = 0.12063323706388474\n",
            "epoch 1/2, step 189 / 235 \n",
            "loss = 0.1348019540309906\n",
            "Validation Loss = 0.11572863906621933\n",
            "epoch 1/2, step 190 / 235 \n",
            "loss = 0.11281445622444153\n",
            "Validation Loss = 0.1448230892419815\n",
            "epoch 1/2, step 191 / 235 \n",
            "loss = 0.14840877056121826\n",
            "Validation Loss = 0.13876548409461975\n",
            "epoch 1/2, step 192 / 235 \n",
            "loss = 0.10680937021970749\n",
            "Validation Loss = 0.1370263546705246\n",
            "epoch 1/2, step 193 / 235 \n",
            "loss = 0.04552562162280083\n",
            "Validation Loss = 0.1377859264612198\n",
            "epoch 1/2, step 194 / 235 \n",
            "loss = 0.0842008963227272\n",
            "Validation Loss = 0.11889249831438065\n",
            "epoch 1/2, step 195 / 235 \n",
            "loss = 0.1841520071029663\n",
            "Validation Loss = 0.12188965082168579\n",
            "epoch 1/2, step 196 / 235 \n",
            "loss = 0.15220622718334198\n",
            "Validation Loss = 0.12165731936693192\n",
            "epoch 1/2, step 197 / 235 \n",
            "loss = 0.1587391048669815\n",
            "Validation Loss = 0.11624284833669662\n",
            "epoch 1/2, step 198 / 235 \n",
            "loss = 0.11981434375047684\n",
            "Validation Loss = 0.11340155452489853\n",
            "epoch 1/2, step 199 / 235 \n",
            "loss = 0.08772094547748566\n",
            "Validation Loss = 0.08883316814899445\n",
            "epoch 1/2, step 200 / 235 \n",
            "loss = 0.12363406270742416\n",
            "Validation Loss = 0.09987770020961761\n",
            "epoch 1/2, step 201 / 235 \n",
            "loss = 0.13026414811611176\n",
            "Validation Loss = 0.10855467617511749\n",
            "epoch 1/2, step 202 / 235 \n",
            "loss = 0.08048154413700104\n",
            "Validation Loss = 0.11188437044620514\n",
            "epoch 1/2, step 203 / 235 \n",
            "loss = 0.17903423309326172\n",
            "Validation Loss = 0.12629728019237518\n",
            "epoch 1/2, step 204 / 235 \n",
            "loss = 0.13779336214065552\n",
            "Validation Loss = 0.16749714314937592\n",
            "epoch 1/2, step 205 / 235 \n",
            "loss = 0.19267401099205017\n",
            "Validation Loss = 0.12592492997646332\n",
            "epoch 1/2, step 206 / 235 \n",
            "loss = 0.15673406422138214\n",
            "Validation Loss = 0.12218058854341507\n",
            "epoch 1/2, step 207 / 235 \n",
            "loss = 0.1615578979253769\n",
            "Validation Loss = 0.1329186111688614\n",
            "epoch 1/2, step 208 / 235 \n",
            "loss = 0.18113335967063904\n",
            "Validation Loss = 0.13983076810836792\n",
            "epoch 1/2, step 209 / 235 \n",
            "loss = 0.14840567111968994\n",
            "Validation Loss = 0.13033075630664825\n",
            "epoch 1/2, step 210 / 235 \n",
            "loss = 0.14654840528964996\n",
            "Validation Loss = 0.10464467108249664\n",
            "epoch 1/2, step 211 / 235 \n",
            "loss = 0.09047152101993561\n",
            "Validation Loss = 0.09661120921373367\n",
            "epoch 1/2, step 212 / 235 \n",
            "loss = 0.06061767786741257\n",
            "Validation Loss = 0.12407545745372772\n",
            "epoch 1/2, step 213 / 235 \n",
            "loss = 0.07143736630678177\n",
            "Validation Loss = 0.1608126014471054\n",
            "epoch 1/2, step 214 / 235 \n",
            "loss = 0.16987945139408112\n",
            "Validation Loss = 0.15198513865470886\n",
            "epoch 1/2, step 215 / 235 \n",
            "loss = 0.14535802602767944\n",
            "Validation Loss = 0.11519774794578552\n",
            "epoch 1/2, step 216 / 235 \n",
            "loss = 0.13537460565567017\n",
            "Validation Loss = 0.09822370111942291\n",
            "epoch 1/2, step 217 / 235 \n",
            "loss = 0.16770245134830475\n",
            "Validation Loss = 0.17052024602890015\n",
            "epoch 1/2, step 218 / 235 \n",
            "loss = 0.12461516261100769\n",
            "Validation Loss = 0.12707147002220154\n",
            "epoch 1/2, step 219 / 235 \n",
            "loss = 0.08675979822874069\n",
            "Validation Loss = 0.1619952768087387\n",
            "epoch 1/2, step 220 / 235 \n",
            "loss = 0.11282459646463394\n",
            "Validation Loss = 0.12542995810508728\n",
            "epoch 1/2, step 221 / 235 \n",
            "loss = 0.133785679936409\n",
            "Validation Loss = 0.1076115295290947\n",
            "epoch 1/2, step 222 / 235 \n",
            "loss = 0.15052492916584015\n",
            "Validation Loss = 0.1448485255241394\n",
            "epoch 1/2, step 223 / 235 \n",
            "loss = 0.10012403875589371\n",
            "Validation Loss = 0.0906078889966011\n",
            "epoch 1/2, step 224 / 235 \n",
            "loss = 0.0757981613278389\n",
            "Validation Loss = 0.12116163969039917\n",
            "epoch 1/2, step 225 / 235 \n",
            "loss = 0.11602625250816345\n",
            "Validation Loss = 0.10073374211788177\n",
            "epoch 1/2, step 226 / 235 \n",
            "loss = 0.06164413318037987\n",
            "Validation Loss = 0.10973691940307617\n",
            "epoch 1/2, step 227 / 235 \n",
            "loss = 0.08793053776025772\n",
            "Validation Loss = 0.1341194361448288\n",
            "epoch 1/2, step 228 / 235 \n",
            "loss = 0.08736435323953629\n",
            "Validation Loss = 0.11961639672517776\n",
            "epoch 1/2, step 229 / 235 \n",
            "loss = 0.13050562143325806\n",
            "Validation Loss = 0.11350733041763306\n",
            "epoch 1/2, step 230 / 235 \n",
            "loss = 0.1318860948085785\n",
            "Validation Loss = 0.1216810792684555\n",
            "epoch 1/2, step 231 / 235 \n",
            "loss = 0.13044299185276031\n",
            "Validation Loss = 0.11176750808954239\n",
            "epoch 1/2, step 232 / 235 \n",
            "loss = 0.16756987571716309\n",
            "Validation Loss = 0.1568933129310608\n",
            "epoch 1/2, step 233 / 235 \n",
            "loss = 0.15662313997745514\n",
            "Validation Loss = 0.1198546290397644\n",
            "epoch 1/2, step 234 / 235 \n",
            "loss = 0.15687215328216553\n",
            "Validation Loss = 0.11313535273075104\n",
            "epoch 1/2, step 235 / 235 \n",
            "loss = 0.053806811571121216\n",
            "Validation Loss = 0.1072319746017456\n",
            "epoch 2/2, step 1 / 235 \n",
            "loss = 1.1108334064483643\n",
            "Validation Loss = 0.11852220445871353\n",
            "epoch 2/2, step 2 / 235 \n",
            "loss = 0.12058452516794205\n",
            "Validation Loss = 0.11960253119468689\n",
            "epoch 2/2, step 3 / 235 \n",
            "loss = 0.1782861202955246\n",
            "Validation Loss = 0.1523376703262329\n",
            "epoch 2/2, step 4 / 235 \n",
            "loss = 0.11937591433525085\n",
            "Validation Loss = 0.12842176854610443\n",
            "epoch 2/2, step 5 / 235 \n",
            "loss = 0.14992783963680267\n",
            "Validation Loss = 0.1373523473739624\n",
            "epoch 2/2, step 6 / 235 \n",
            "loss = 0.11575528234243393\n",
            "Validation Loss = 0.1502823382616043\n",
            "epoch 2/2, step 7 / 235 \n",
            "loss = 0.08598113059997559\n",
            "Validation Loss = 0.10936091095209122\n",
            "epoch 2/2, step 8 / 235 \n",
            "loss = 0.14452092349529266\n",
            "Validation Loss = 0.11081821471452713\n",
            "epoch 2/2, step 9 / 235 \n",
            "loss = 0.1343427300453186\n",
            "Validation Loss = 0.12353330105543137\n",
            "epoch 2/2, step 10 / 235 \n",
            "loss = 0.13612596690654755\n",
            "Validation Loss = 0.11229703575372696\n",
            "epoch 2/2, step 11 / 235 \n",
            "loss = 0.09612952917814255\n",
            "Validation Loss = 0.12446365505456924\n",
            "epoch 2/2, step 12 / 235 \n",
            "loss = 0.18340186774730682\n",
            "Validation Loss = 0.14890609681606293\n",
            "epoch 2/2, step 13 / 235 \n",
            "loss = 0.1192433089017868\n",
            "Validation Loss = 0.15295296907424927\n",
            "epoch 2/2, step 14 / 235 \n",
            "loss = 0.09856540709733963\n",
            "Validation Loss = 0.12333191186189651\n",
            "epoch 2/2, step 15 / 235 \n",
            "loss = 0.11889752000570297\n",
            "Validation Loss = 0.1272832155227661\n",
            "epoch 2/2, step 16 / 235 \n",
            "loss = 0.11619419604539871\n",
            "Validation Loss = 0.1123826652765274\n",
            "epoch 2/2, step 17 / 235 \n",
            "loss = 0.06757360696792603\n",
            "Validation Loss = 0.11979140341281891\n",
            "epoch 2/2, step 18 / 235 \n",
            "loss = 0.1152985468506813\n",
            "Validation Loss = 0.13060039281845093\n",
            "epoch 2/2, step 19 / 235 \n",
            "loss = 0.12576864659786224\n",
            "Validation Loss = 0.09631112962961197\n",
            "epoch 2/2, step 20 / 235 \n",
            "loss = 0.05873529240489006\n",
            "Validation Loss = 0.11849644035100937\n",
            "epoch 2/2, step 21 / 235 \n",
            "loss = 0.0866360142827034\n",
            "Validation Loss = 0.12008274346590042\n",
            "epoch 2/2, step 22 / 235 \n",
            "loss = 0.10158339142799377\n",
            "Validation Loss = 0.13721807301044464\n",
            "epoch 2/2, step 23 / 235 \n",
            "loss = 0.1200379952788353\n",
            "Validation Loss = 0.13071992993354797\n",
            "epoch 2/2, step 24 / 235 \n",
            "loss = 0.11257433146238327\n",
            "Validation Loss = 0.15584716200828552\n",
            "epoch 2/2, step 25 / 235 \n",
            "loss = 0.14455492794513702\n",
            "Validation Loss = 0.10867157578468323\n",
            "epoch 2/2, step 26 / 235 \n",
            "loss = 0.1383618712425232\n",
            "Validation Loss = 0.11065332591533661\n",
            "epoch 2/2, step 27 / 235 \n",
            "loss = 0.10130894184112549\n",
            "Validation Loss = 0.09872309863567352\n",
            "epoch 2/2, step 28 / 235 \n",
            "loss = 0.1432286947965622\n",
            "Validation Loss = 0.1088438630104065\n",
            "epoch 2/2, step 29 / 235 \n",
            "loss = 0.12472986429929733\n",
            "Validation Loss = 0.12211023271083832\n",
            "epoch 2/2, step 30 / 235 \n",
            "loss = 0.13676941394805908\n",
            "Validation Loss = 0.11983223259449005\n",
            "epoch 2/2, step 31 / 235 \n",
            "loss = 0.08055705577135086\n",
            "Validation Loss = 0.11401482671499252\n",
            "epoch 2/2, step 32 / 235 \n",
            "loss = 0.0931316539645195\n",
            "Validation Loss = 0.1051073968410492\n",
            "epoch 2/2, step 33 / 235 \n",
            "loss = 0.10590049624443054\n",
            "Validation Loss = 0.13617634773254395\n",
            "epoch 2/2, step 34 / 235 \n",
            "loss = 0.08929291367530823\n",
            "Validation Loss = 0.10088074952363968\n",
            "epoch 2/2, step 35 / 235 \n",
            "loss = 0.059829097241163254\n",
            "Validation Loss = 0.06061315909028053\n",
            "epoch 2/2, step 36 / 235 \n",
            "loss = 0.10002943128347397\n",
            "Validation Loss = 0.12217970937490463\n",
            "epoch 2/2, step 37 / 235 \n",
            "loss = 0.08684878796339035\n",
            "Validation Loss = 0.12076600641012192\n",
            "epoch 2/2, step 38 / 235 \n",
            "loss = 0.08805979043245316\n",
            "Validation Loss = 0.1330532282590866\n",
            "epoch 2/2, step 39 / 235 \n",
            "loss = 0.11268164217472076\n",
            "Validation Loss = 0.1123841404914856\n",
            "epoch 2/2, step 40 / 235 \n",
            "loss = 0.1439862698316574\n",
            "Validation Loss = 0.12439063936471939\n",
            "epoch 2/2, step 41 / 235 \n",
            "loss = 0.08386510610580444\n",
            "Validation Loss = 0.10899660736322403\n",
            "epoch 2/2, step 42 / 235 \n",
            "loss = 0.09400682151317596\n",
            "Validation Loss = 0.1110980361700058\n",
            "epoch 2/2, step 43 / 235 \n",
            "loss = 0.11226816475391388\n",
            "Validation Loss = 0.10409142822027206\n",
            "epoch 2/2, step 44 / 235 \n",
            "loss = 0.09779217839241028\n",
            "Validation Loss = 0.1395585834980011\n",
            "epoch 2/2, step 45 / 235 \n",
            "loss = 0.11026989668607712\n",
            "Validation Loss = 0.10643591731786728\n",
            "epoch 2/2, step 46 / 235 \n",
            "loss = 0.10218659043312073\n",
            "Validation Loss = 0.10096057504415512\n",
            "epoch 2/2, step 47 / 235 \n",
            "loss = 0.08094736188650131\n",
            "Validation Loss = 0.1058923602104187\n",
            "epoch 2/2, step 48 / 235 \n",
            "loss = 0.08733266592025757\n",
            "Validation Loss = 0.12120749801397324\n",
            "epoch 2/2, step 49 / 235 \n",
            "loss = 0.10527092963457108\n",
            "Validation Loss = 0.1502908170223236\n",
            "epoch 2/2, step 50 / 235 \n",
            "loss = 0.09433523565530777\n",
            "Validation Loss = 0.11585839837789536\n",
            "epoch 2/2, step 51 / 235 \n",
            "loss = 0.1002211794257164\n",
            "Validation Loss = 0.10384827107191086\n",
            "epoch 2/2, step 52 / 235 \n",
            "loss = 0.055797576904296875\n",
            "Validation Loss = 0.13048134744167328\n",
            "epoch 2/2, step 53 / 235 \n",
            "loss = 0.09900747984647751\n",
            "Validation Loss = 0.10486943274736404\n",
            "epoch 2/2, step 54 / 235 \n",
            "loss = 0.13106103241443634\n",
            "Validation Loss = 0.07863481342792511\n",
            "epoch 2/2, step 55 / 235 \n",
            "loss = 0.07591859251260757\n",
            "Validation Loss = 0.1082616075873375\n",
            "epoch 2/2, step 56 / 235 \n",
            "loss = 0.08997523039579391\n",
            "Validation Loss = 0.12623657286167145\n",
            "epoch 2/2, step 57 / 235 \n",
            "loss = 0.08138340711593628\n",
            "Validation Loss = 0.14063400030136108\n",
            "epoch 2/2, step 58 / 235 \n",
            "loss = 0.1011873334646225\n",
            "Validation Loss = 0.16636048257350922\n",
            "epoch 2/2, step 59 / 235 \n",
            "loss = 0.1386815309524536\n",
            "Validation Loss = 0.120254747569561\n",
            "epoch 2/2, step 60 / 235 \n",
            "loss = 0.13065744936466217\n",
            "Validation Loss = 0.12748561799526215\n",
            "epoch 2/2, step 61 / 235 \n",
            "loss = 0.13214921951293945\n",
            "Validation Loss = 0.08660995960235596\n",
            "epoch 2/2, step 62 / 235 \n",
            "loss = 0.12019006907939911\n",
            "Validation Loss = 0.08093257248401642\n",
            "epoch 2/2, step 63 / 235 \n",
            "loss = 0.059395916759967804\n",
            "Validation Loss = 0.09936867654323578\n",
            "epoch 2/2, step 64 / 235 \n",
            "loss = 0.08759025484323502\n",
            "Validation Loss = 0.11850214004516602\n",
            "epoch 2/2, step 65 / 235 \n",
            "loss = 0.15169371664524078\n",
            "Validation Loss = 0.1292039155960083\n",
            "epoch 2/2, step 66 / 235 \n",
            "loss = 0.12075511366128922\n",
            "Validation Loss = 0.13027849793434143\n",
            "epoch 2/2, step 67 / 235 \n",
            "loss = 0.10303438454866409\n",
            "Validation Loss = 0.17411679029464722\n",
            "epoch 2/2, step 68 / 235 \n",
            "loss = 0.0972888171672821\n",
            "Validation Loss = 0.1576274186372757\n",
            "epoch 2/2, step 69 / 235 \n",
            "loss = 0.11549586802721024\n",
            "Validation Loss = 0.13589106500148773\n",
            "epoch 2/2, step 70 / 235 \n",
            "loss = 0.12280958145856857\n",
            "Validation Loss = 0.15688900649547577\n",
            "epoch 2/2, step 71 / 235 \n",
            "loss = 0.10680651664733887\n",
            "Validation Loss = 0.18094290792942047\n",
            "epoch 2/2, step 72 / 235 \n",
            "loss = 0.0914233922958374\n",
            "Validation Loss = 0.1217675432562828\n",
            "epoch 2/2, step 73 / 235 \n",
            "loss = 0.15007224678993225\n",
            "Validation Loss = 0.11183182895183563\n",
            "epoch 2/2, step 74 / 235 \n",
            "loss = 0.18273480236530304\n",
            "Validation Loss = 0.12389623373746872\n",
            "epoch 2/2, step 75 / 235 \n",
            "loss = 0.1387699544429779\n",
            "Validation Loss = 0.14272156357765198\n",
            "epoch 2/2, step 76 / 235 \n",
            "loss = 0.08511390537023544\n",
            "Validation Loss = 0.1391364336013794\n",
            "epoch 2/2, step 77 / 235 \n",
            "loss = 0.12075015157461166\n",
            "Validation Loss = 0.13525141775608063\n",
            "epoch 2/2, step 78 / 235 \n",
            "loss = 0.09368625283241272\n",
            "Validation Loss = 0.13845765590667725\n",
            "epoch 2/2, step 79 / 235 \n",
            "loss = 0.14206711947917938\n",
            "Validation Loss = 0.1076902523636818\n",
            "epoch 2/2, step 80 / 235 \n",
            "loss = 0.14566950500011444\n",
            "Validation Loss = 0.10341713577508926\n",
            "epoch 2/2, step 81 / 235 \n",
            "loss = 0.10223981738090515\n",
            "Validation Loss = 0.10467120260000229\n",
            "epoch 2/2, step 82 / 235 \n",
            "loss = 0.15487010776996613\n",
            "Validation Loss = 0.1014246866106987\n",
            "epoch 2/2, step 83 / 235 \n",
            "loss = 0.061827052384614944\n",
            "Validation Loss = 0.08859934657812119\n",
            "epoch 2/2, step 84 / 235 \n",
            "loss = 0.09162911772727966\n",
            "Validation Loss = 0.1275719702243805\n",
            "epoch 2/2, step 85 / 235 \n",
            "loss = 0.130324125289917\n",
            "Validation Loss = 0.14015164971351624\n",
            "epoch 2/2, step 86 / 235 \n",
            "loss = 0.10193970054388046\n",
            "Validation Loss = 0.14396442472934723\n",
            "epoch 2/2, step 87 / 235 \n",
            "loss = 0.08981011807918549\n",
            "Validation Loss = 0.17071108520030975\n",
            "epoch 2/2, step 88 / 235 \n",
            "loss = 0.12862540781497955\n",
            "Validation Loss = 0.14591331779956818\n",
            "epoch 2/2, step 89 / 235 \n",
            "loss = 0.09911757707595825\n",
            "Validation Loss = 0.13663490116596222\n",
            "epoch 2/2, step 90 / 235 \n",
            "loss = 0.11928850412368774\n",
            "Validation Loss = 0.12232834845781326\n",
            "epoch 2/2, step 91 / 235 \n",
            "loss = 0.09436237066984177\n",
            "Validation Loss = 0.09433088451623917\n",
            "epoch 2/2, step 92 / 235 \n",
            "loss = 0.07330698519945145\n",
            "Validation Loss = 0.10744433850049973\n",
            "epoch 2/2, step 93 / 235 \n",
            "loss = 0.126978799700737\n",
            "Validation Loss = 0.09274881333112717\n",
            "epoch 2/2, step 94 / 235 \n",
            "loss = 0.0984058827161789\n",
            "Validation Loss = 0.11868840456008911\n",
            "epoch 2/2, step 95 / 235 \n",
            "loss = 0.1415146142244339\n",
            "Validation Loss = 0.10498430579900742\n",
            "epoch 2/2, step 96 / 235 \n",
            "loss = 0.11402377486228943\n",
            "Validation Loss = 0.1473165601491928\n",
            "epoch 2/2, step 97 / 235 \n",
            "loss = 0.1404573619365692\n",
            "Validation Loss = 0.11006589978933334\n",
            "epoch 2/2, step 98 / 235 \n",
            "loss = 0.12453336268663406\n",
            "Validation Loss = 0.1260988861322403\n",
            "epoch 2/2, step 99 / 235 \n",
            "loss = 0.11152546107769012\n",
            "Validation Loss = 0.0948147103190422\n",
            "epoch 2/2, step 100 / 235 \n",
            "loss = 0.10167929530143738\n",
            "Validation Loss = 0.11627796292304993\n",
            "epoch 2/2, step 101 / 235 \n",
            "loss = 0.1335562765598297\n",
            "Validation Loss = 0.08715444803237915\n",
            "epoch 2/2, step 102 / 235 \n",
            "loss = 0.11732161045074463\n",
            "Validation Loss = 0.10496921837329865\n",
            "epoch 2/2, step 103 / 235 \n",
            "loss = 0.09130910038948059\n",
            "Validation Loss = 0.14247265458106995\n",
            "epoch 2/2, step 104 / 235 \n",
            "loss = 0.111879862844944\n",
            "Validation Loss = 0.13482223451137543\n",
            "epoch 2/2, step 105 / 235 \n",
            "loss = 0.11925778537988663\n",
            "Validation Loss = 0.07756078988313675\n",
            "epoch 2/2, step 106 / 235 \n",
            "loss = 0.10583169013261795\n",
            "Validation Loss = 0.09574670344591141\n",
            "epoch 2/2, step 107 / 235 \n",
            "loss = 0.10752353072166443\n",
            "Validation Loss = 0.09311909973621368\n",
            "epoch 2/2, step 108 / 235 \n",
            "loss = 0.12186401337385178\n",
            "Validation Loss = 0.13688652217388153\n",
            "epoch 2/2, step 109 / 235 \n",
            "loss = 0.1399780809879303\n",
            "Validation Loss = 0.11589446663856506\n",
            "epoch 2/2, step 110 / 235 \n",
            "loss = 0.17046815156936646\n",
            "Validation Loss = 0.10927028208971024\n",
            "epoch 2/2, step 111 / 235 \n",
            "loss = 0.13072963058948517\n",
            "Validation Loss = 0.0893590897321701\n",
            "epoch 2/2, step 112 / 235 \n",
            "loss = 0.07285129278898239\n",
            "Validation Loss = 0.12038953602313995\n",
            "epoch 2/2, step 113 / 235 \n",
            "loss = 0.1255960315465927\n",
            "Validation Loss = 0.1703561693429947\n",
            "epoch 2/2, step 114 / 235 \n",
            "loss = 0.13837026059627533\n",
            "Validation Loss = 0.1350325345993042\n",
            "epoch 2/2, step 115 / 235 \n",
            "loss = 0.16813217103481293\n",
            "Validation Loss = 0.10150156170129776\n",
            "epoch 2/2, step 116 / 235 \n",
            "loss = 0.15158142149448395\n",
            "Validation Loss = 0.1134655773639679\n",
            "epoch 2/2, step 117 / 235 \n",
            "loss = 0.12168595939874649\n",
            "Validation Loss = 0.13892623782157898\n",
            "epoch 2/2, step 118 / 235 \n",
            "loss = 0.09802408516407013\n",
            "Validation Loss = 0.11062488704919815\n",
            "epoch 2/2, step 119 / 235 \n",
            "loss = 0.18021705746650696\n",
            "Validation Loss = 0.13571707904338837\n",
            "epoch 2/2, step 120 / 235 \n",
            "loss = 0.19867858290672302\n",
            "Validation Loss = 0.12294899672269821\n",
            "epoch 2/2, step 121 / 235 \n",
            "loss = 0.0689883604645729\n",
            "Validation Loss = 0.13506107032299042\n",
            "epoch 2/2, step 122 / 235 \n",
            "loss = 0.09069523960351944\n",
            "Validation Loss = 0.17680500447750092\n",
            "epoch 2/2, step 123 / 235 \n",
            "loss = 0.06123168393969536\n",
            "Validation Loss = 0.16025452315807343\n",
            "epoch 2/2, step 124 / 235 \n",
            "loss = 0.19549551606178284\n",
            "Validation Loss = 0.17032699286937714\n",
            "epoch 2/2, step 125 / 235 \n",
            "loss = 0.1179698035120964\n",
            "Validation Loss = 0.15859206020832062\n",
            "epoch 2/2, step 126 / 235 \n",
            "loss = 0.10604101419448853\n",
            "Validation Loss = 0.1353214830160141\n",
            "epoch 2/2, step 127 / 235 \n",
            "loss = 0.14205697178840637\n",
            "Validation Loss = 0.13272853195667267\n",
            "epoch 2/2, step 128 / 235 \n",
            "loss = 0.06660196930170059\n",
            "Validation Loss = 0.08891192823648453\n",
            "epoch 2/2, step 129 / 235 \n",
            "loss = 0.07284320890903473\n",
            "Validation Loss = 0.10646606236696243\n",
            "epoch 2/2, step 130 / 235 \n",
            "loss = 0.11396612972021103\n",
            "Validation Loss = 0.14961256086826324\n",
            "epoch 2/2, step 131 / 235 \n",
            "loss = 0.07777754962444305\n",
            "Validation Loss = 0.1951918601989746\n",
            "epoch 2/2, step 132 / 235 \n",
            "loss = 0.2037408947944641\n",
            "Validation Loss = 0.12328890711069107\n",
            "epoch 2/2, step 133 / 235 \n",
            "loss = 0.14234498143196106\n",
            "Validation Loss = 0.11884398758411407\n",
            "epoch 2/2, step 134 / 235 \n",
            "loss = 0.14544270932674408\n",
            "Validation Loss = 0.17044048011302948\n",
            "epoch 2/2, step 135 / 235 \n",
            "loss = 0.18401333689689636\n",
            "Validation Loss = 0.18042059242725372\n",
            "epoch 2/2, step 136 / 235 \n",
            "loss = 0.13529174029827118\n",
            "Validation Loss = 0.2111126333475113\n",
            "epoch 2/2, step 137 / 235 \n",
            "loss = 0.16522018611431122\n",
            "Validation Loss = 0.22855328023433685\n",
            "epoch 2/2, step 138 / 235 \n",
            "loss = 0.2388952076435089\n",
            "Validation Loss = 0.2541253864765167\n",
            "epoch 2/2, step 139 / 235 \n",
            "loss = 0.14006537199020386\n",
            "Validation Loss = 0.16197188198566437\n",
            "epoch 2/2, step 140 / 235 \n",
            "loss = 0.2008710652589798\n",
            "Validation Loss = 0.1677686870098114\n",
            "epoch 2/2, step 141 / 235 \n",
            "loss = 0.13727933168411255\n",
            "Validation Loss = 0.1862116903066635\n",
            "epoch 2/2, step 142 / 235 \n",
            "loss = 0.24511770904064178\n",
            "Validation Loss = 0.20429086685180664\n",
            "epoch 2/2, step 143 / 235 \n",
            "loss = 0.17284131050109863\n",
            "Validation Loss = 0.1481529176235199\n",
            "epoch 2/2, step 144 / 235 \n",
            "loss = 0.15255479514598846\n",
            "Validation Loss = 0.11197521537542343\n",
            "epoch 2/2, step 145 / 235 \n",
            "loss = 0.15594929456710815\n",
            "Validation Loss = 0.1307002305984497\n",
            "epoch 2/2, step 146 / 235 \n",
            "loss = 0.11032663285732269\n",
            "Validation Loss = 0.16496136784553528\n",
            "epoch 2/2, step 147 / 235 \n",
            "loss = 0.1655270904302597\n",
            "Validation Loss = 0.23429037630558014\n",
            "epoch 2/2, step 148 / 235 \n",
            "loss = 0.21788997948169708\n",
            "Validation Loss = 0.1676749289035797\n",
            "epoch 2/2, step 149 / 235 \n",
            "loss = 0.16560512781143188\n",
            "Validation Loss = 0.17163729667663574\n",
            "epoch 2/2, step 150 / 235 \n",
            "loss = 0.220304474234581\n",
            "Validation Loss = 0.10701004415750504\n",
            "epoch 2/2, step 151 / 235 \n",
            "loss = 0.18687832355499268\n",
            "Validation Loss = 0.182825967669487\n",
            "epoch 2/2, step 152 / 235 \n",
            "loss = 0.13923373818397522\n",
            "Validation Loss = 0.15771479904651642\n",
            "epoch 2/2, step 153 / 235 \n",
            "loss = 0.15299926698207855\n",
            "Validation Loss = 0.19458699226379395\n",
            "epoch 2/2, step 154 / 235 \n",
            "loss = 0.17105653882026672\n",
            "Validation Loss = 0.18022482097148895\n",
            "epoch 2/2, step 155 / 235 \n",
            "loss = 0.25807690620422363\n",
            "Validation Loss = 0.21487437188625336\n",
            "epoch 2/2, step 156 / 235 \n",
            "loss = 0.16756263375282288\n",
            "Validation Loss = 0.20451605319976807\n",
            "epoch 2/2, step 157 / 235 \n",
            "loss = 0.2237735241651535\n",
            "Validation Loss = 0.1743774563074112\n",
            "epoch 2/2, step 158 / 235 \n",
            "loss = 0.15118061006069183\n",
            "Validation Loss = 0.1446981132030487\n",
            "epoch 2/2, step 159 / 235 \n",
            "loss = 0.12961144745349884\n",
            "Validation Loss = 0.17787958681583405\n",
            "epoch 2/2, step 160 / 235 \n",
            "loss = 0.1377466470003128\n",
            "Validation Loss = 0.1724412441253662\n",
            "epoch 2/2, step 161 / 235 \n",
            "loss = 0.14496517181396484\n",
            "Validation Loss = 0.18203924596309662\n",
            "epoch 2/2, step 162 / 235 \n",
            "loss = 0.17317575216293335\n",
            "Validation Loss = 0.15074333548545837\n",
            "epoch 2/2, step 163 / 235 \n",
            "loss = 0.10538412630558014\n",
            "Validation Loss = 0.12860533595085144\n",
            "epoch 2/2, step 164 / 235 \n",
            "loss = 0.13051334023475647\n",
            "Validation Loss = 0.14674493670463562\n",
            "epoch 2/2, step 165 / 235 \n",
            "loss = 0.12453571707010269\n",
            "Validation Loss = 0.13659431040287018\n",
            "epoch 2/2, step 166 / 235 \n",
            "loss = 0.11662625521421432\n",
            "Validation Loss = 0.13470697402954102\n",
            "epoch 2/2, step 167 / 235 \n",
            "loss = 0.13805700838565826\n",
            "Validation Loss = 0.10888849198818207\n",
            "epoch 2/2, step 168 / 235 \n",
            "loss = 0.12503567337989807\n",
            "Validation Loss = 0.15194344520568848\n",
            "epoch 2/2, step 169 / 235 \n",
            "loss = 0.14551600813865662\n",
            "Validation Loss = 0.1593220829963684\n",
            "epoch 2/2, step 170 / 235 \n",
            "loss = 0.10252631455659866\n",
            "Validation Loss = 0.10281816124916077\n",
            "epoch 2/2, step 171 / 235 \n",
            "loss = 0.09434712678194046\n",
            "Validation Loss = 0.09525406360626221\n",
            "epoch 2/2, step 172 / 235 \n",
            "loss = 0.10002440214157104\n",
            "Validation Loss = 0.16476011276245117\n",
            "epoch 2/2, step 173 / 235 \n",
            "loss = 0.0767478197813034\n",
            "Validation Loss = 0.11810750514268875\n",
            "epoch 2/2, step 174 / 235 \n",
            "loss = 0.1375456005334854\n",
            "Validation Loss = 0.11977536976337433\n",
            "epoch 2/2, step 175 / 235 \n",
            "loss = 0.1341835856437683\n",
            "Validation Loss = 0.14041969180107117\n",
            "epoch 2/2, step 176 / 235 \n",
            "loss = 0.14883995056152344\n",
            "Validation Loss = 0.1595633178949356\n",
            "epoch 2/2, step 177 / 235 \n",
            "loss = 0.16786998510360718\n",
            "Validation Loss = 0.12408720701932907\n",
            "epoch 2/2, step 178 / 235 \n",
            "loss = 0.1319963037967682\n",
            "Validation Loss = 0.1038479208946228\n",
            "epoch 2/2, step 179 / 235 \n",
            "loss = 0.06467359513044357\n",
            "Validation Loss = 0.11365208029747009\n",
            "epoch 2/2, step 180 / 235 \n",
            "loss = 0.11175630241632462\n",
            "Validation Loss = 0.10962266474962234\n",
            "epoch 2/2, step 181 / 235 \n",
            "loss = 0.14592410624027252\n",
            "Validation Loss = 0.12868092954158783\n",
            "epoch 2/2, step 182 / 235 \n",
            "loss = 0.10359864681959152\n",
            "Validation Loss = 0.1057095155119896\n",
            "epoch 2/2, step 183 / 235 \n",
            "loss = 0.1523941159248352\n",
            "Validation Loss = 0.1321558952331543\n",
            "epoch 2/2, step 184 / 235 \n",
            "loss = 0.09475977718830109\n",
            "Validation Loss = 0.11293574422597885\n",
            "epoch 2/2, step 185 / 235 \n",
            "loss = 0.09956281632184982\n",
            "Validation Loss = 0.09596607834100723\n",
            "epoch 2/2, step 186 / 235 \n",
            "loss = 0.08269944041967392\n",
            "Validation Loss = 0.09345492720603943\n",
            "epoch 2/2, step 187 / 235 \n",
            "loss = 0.11665718257427216\n",
            "Validation Loss = 0.17379720509052277\n",
            "epoch 2/2, step 188 / 235 \n",
            "loss = 0.10812122374773026\n",
            "Validation Loss = 0.15941505134105682\n",
            "epoch 2/2, step 189 / 235 \n",
            "loss = 0.1185164675116539\n",
            "Validation Loss = 0.127157524228096\n",
            "epoch 2/2, step 190 / 235 \n",
            "loss = 0.09229965507984161\n",
            "Validation Loss = 0.177313432097435\n",
            "epoch 2/2, step 191 / 235 \n",
            "loss = 0.11663901060819626\n",
            "Validation Loss = 0.11896553635597229\n",
            "epoch 2/2, step 192 / 235 \n",
            "loss = 0.11317113041877747\n",
            "Validation Loss = 0.09792473912239075\n",
            "epoch 2/2, step 193 / 235 \n",
            "loss = 0.09205566346645355\n",
            "Validation Loss = 0.10619261115789413\n",
            "epoch 2/2, step 194 / 235 \n",
            "loss = 0.08844465017318726\n",
            "Validation Loss = 0.1036512702703476\n",
            "epoch 2/2, step 195 / 235 \n",
            "loss = 0.08753926306962967\n",
            "Validation Loss = 0.08300638943910599\n",
            "epoch 2/2, step 196 / 235 \n",
            "loss = 0.07764049619436264\n",
            "Validation Loss = 0.14915873110294342\n",
            "epoch 2/2, step 197 / 235 \n",
            "loss = 0.15909825265407562\n",
            "Validation Loss = 0.10482599586248398\n",
            "epoch 2/2, step 198 / 235 \n",
            "loss = 0.09892789274454117\n",
            "Validation Loss = 0.1337280571460724\n",
            "epoch 2/2, step 199 / 235 \n",
            "loss = 0.13877131044864655\n",
            "Validation Loss = 0.09622219949960709\n",
            "epoch 2/2, step 200 / 235 \n",
            "loss = 0.15023913979530334\n",
            "Validation Loss = 0.06719661504030228\n",
            "epoch 2/2, step 201 / 235 \n",
            "loss = 0.056966349482536316\n",
            "Validation Loss = 0.09877090156078339\n",
            "epoch 2/2, step 202 / 235 \n",
            "loss = 0.14704473316669464\n",
            "Validation Loss = 0.11567872017621994\n",
            "epoch 2/2, step 203 / 235 \n",
            "loss = 0.1259033977985382\n",
            "Validation Loss = 0.10489271581172943\n",
            "epoch 2/2, step 204 / 235 \n",
            "loss = 0.09052178263664246\n",
            "Validation Loss = 0.17274026572704315\n",
            "epoch 2/2, step 205 / 235 \n",
            "loss = 0.1261812001466751\n",
            "Validation Loss = 0.09181978553533554\n",
            "epoch 2/2, step 206 / 235 \n",
            "loss = 0.10868632793426514\n",
            "Validation Loss = 0.07257763296365738\n",
            "epoch 2/2, step 207 / 235 \n",
            "loss = 0.10390686243772507\n",
            "Validation Loss = 0.10774906724691391\n",
            "epoch 2/2, step 208 / 235 \n",
            "loss = 0.10448392480611801\n",
            "Validation Loss = 0.13062046468257904\n",
            "epoch 2/2, step 209 / 235 \n",
            "loss = 0.14180292189121246\n",
            "Validation Loss = 0.174635112285614\n",
            "epoch 2/2, step 210 / 235 \n",
            "loss = 0.11134542524814606\n",
            "Validation Loss = 0.11403051763772964\n",
            "epoch 2/2, step 211 / 235 \n",
            "loss = 0.11732298880815506\n",
            "Validation Loss = 0.11753999441862106\n",
            "epoch 2/2, step 212 / 235 \n",
            "loss = 0.14271381497383118\n",
            "Validation Loss = 0.10247780382633209\n",
            "epoch 2/2, step 213 / 235 \n",
            "loss = 0.0702722892165184\n",
            "Validation Loss = 0.10174999386072159\n",
            "epoch 2/2, step 214 / 235 \n",
            "loss = 0.06816687434911728\n",
            "Validation Loss = 0.0898752436041832\n",
            "epoch 2/2, step 215 / 235 \n",
            "loss = 0.05510934814810753\n",
            "Validation Loss = 0.06894100457429886\n",
            "epoch 2/2, step 216 / 235 \n",
            "loss = 0.07534026354551315\n",
            "Validation Loss = 0.16672368347644806\n",
            "epoch 2/2, step 217 / 235 \n",
            "loss = 0.10099346190690994\n",
            "Validation Loss = 0.15113668143749237\n",
            "epoch 2/2, step 218 / 235 \n",
            "loss = 0.0855170488357544\n",
            "Validation Loss = 0.09229177981615067\n",
            "epoch 2/2, step 219 / 235 \n",
            "loss = 0.09405676275491714\n",
            "Validation Loss = 0.06279969960451126\n",
            "epoch 2/2, step 220 / 235 \n",
            "loss = 0.06364390254020691\n",
            "Validation Loss = 0.10629504173994064\n",
            "epoch 2/2, step 221 / 235 \n",
            "loss = 0.11539481580257416\n",
            "Validation Loss = 0.11250608414411545\n",
            "epoch 2/2, step 222 / 235 \n",
            "loss = 0.06756072491407394\n",
            "Validation Loss = 0.10588598996400833\n",
            "epoch 2/2, step 223 / 235 \n",
            "loss = 0.06407850235700607\n",
            "Validation Loss = 0.08575908839702606\n",
            "epoch 2/2, step 224 / 235 \n",
            "loss = 0.12081563472747803\n",
            "Validation Loss = 0.09086652100086212\n",
            "epoch 2/2, step 225 / 235 \n",
            "loss = 0.10430426150560379\n",
            "Validation Loss = 0.13705269992351532\n",
            "epoch 2/2, step 226 / 235 \n",
            "loss = 0.14129360020160675\n",
            "Validation Loss = 0.1313071995973587\n",
            "epoch 2/2, step 227 / 235 \n",
            "loss = 0.12155185639858246\n",
            "Validation Loss = 0.15486256778240204\n",
            "epoch 2/2, step 228 / 235 \n",
            "loss = 0.08723428100347519\n",
            "Validation Loss = 0.1003083661198616\n",
            "epoch 2/2, step 229 / 235 \n",
            "loss = 0.1207347959280014\n",
            "Validation Loss = 0.10367628186941147\n",
            "epoch 2/2, step 230 / 235 \n",
            "loss = 0.07607536762952805\n",
            "Validation Loss = 0.11459293961524963\n",
            "epoch 2/2, step 231 / 235 \n",
            "loss = 0.08332499116659164\n",
            "Validation Loss = 0.11458012461662292\n",
            "epoch 2/2, step 232 / 235 \n",
            "loss = 0.07102533429861069\n",
            "Validation Loss = 0.12144403904676437\n",
            "epoch 2/2, step 233 / 235 \n",
            "loss = 0.16834576427936554\n",
            "Validation Loss = 0.12167319655418396\n",
            "epoch 2/2, step 234 / 235 \n",
            "loss = 0.10939805209636688\n",
            "Validation Loss = 0.10493897646665573\n",
            "epoch 2/2, step 235 / 235 \n",
            "loss = 0.1491096466779709\n",
            "Validation Loss = 0.11754278838634491\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_accuracy(model, dataloader, device):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient calculation for evaluation\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
        "\n",
        "            outputs = model(inputs)  # Forward pass\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)  # Get the predicted class with the highest probability\n",
        "\n",
        "            total += labels.size(0)  # Increment the total number of samples\n",
        "            correct += (predicted == labels).sum().item()  # Increment the number of correct predictions\n",
        "\n",
        "    accuracy = (correct / total) * 100  # Calculate accuracy as a percentage\n",
        "    return accuracy\n",
        "\n",
        "calculate_accuracy(model, test_loader, 'cpu')"
      ],
      "metadata": {
        "id": "b-d88vU6k5WQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4dd402-5793-4d5e-c585-79f4cc1e1ff9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "96.52"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print(len(val_losses))\n",
        "print(len(train_losses))\n",
        "\n",
        "plt.plot(list(range(len(train_losses))),val_losses, label='val curve')\n",
        "plt.plot(list(range(len(train_losses))),train_losses, label='loss curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "honp_oVFUwjq",
        "outputId": "aeffc0ff-cdb0-44bb-c7db-e6f993334987"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "470\n",
            "470\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-dI_knlV_h5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}