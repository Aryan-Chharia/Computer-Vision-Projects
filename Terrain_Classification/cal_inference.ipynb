{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference rate for randomly created images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agraw\\AppData\\Local\\Temp\\ipykernel_24348\\718533513.py:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CPU inference time: 64.54 ms\n",
      "Average GPU inference time: 10.45 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def load_model(model_path):\n",
    "    # Load the pre-trained model\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(num_ftrs, 5)\n",
    "    \n",
    "    # Load the trained weights\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def generate_random_images(num_images, image_size=224):\n",
    "    return [Image.fromarray(np.random.randint(0, 256, (image_size, image_size, 3), dtype=np.uint8)) for _ in range(num_images)]\n",
    "\n",
    "def measure_inference_time(model, images, preprocess, device, num_runs=100):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    for image in images:\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        _ = model(input_tensor)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        for image in images:\n",
    "            input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "            _ = model(input_tensor)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / (num_runs * len(images))\n",
    "    return avg_time\n",
    "\n",
    "def main():\n",
    "    model_path = 'transfer_learning_models/efficientnet_terrain_classifier.pth'\n",
    "    num_images = 10\n",
    "    num_runs = 100\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    preprocess = preprocess_image()\n",
    "    images = generate_random_images(num_images)\n",
    "    \n",
    "    # Test on CPU\n",
    "    cpu_time = measure_inference_time(model, images, preprocess, torch.device('cpu'), num_runs)\n",
    "    print(f\"Average CPU inference time: {cpu_time*1000:.2f} ms\")\n",
    "    \n",
    "    # Test on GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_time = measure_inference_time(model, images, preprocess, torch.device('cuda'), num_runs)\n",
    "        print(f\"Average GPU inference time: {gpu_time*1000:.2f} ms\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. GPU inference time not measured.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference rate for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agraw\\AppData\\Local\\Temp\\ipykernel_13804\\1084491709.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "Average CPU inference time: 89.56 ms\n",
      "Average GPU inference time: 15.36 ms\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import transforms, models, datasets\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "from torchsummary import summary\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(num_ftrs, 5)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def preprocess_image(image_size=224):\n",
    "    return transforms.Compose([\n",
    "        transforms.Resize((image_size, image_size)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "def load_test_images(test_dir, num_images=10):\n",
    "    test_dataset = datasets.ImageFolder(root=test_dir, transform=None)\n",
    "    images = []\n",
    "    for i in range(min(num_images, len(test_dataset))):\n",
    "        img_path, _ = test_dataset.samples[i]\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def measure_inference_time(model, images, preprocess, device, num_runs=100):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Warm-up run\n",
    "    for image in images:\n",
    "        input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "        _ = model(input_tensor)\n",
    "    \n",
    "    # Measure inference time\n",
    "    start_time = time.time()\n",
    "    for _ in range(num_runs):\n",
    "        for image in images:\n",
    "            input_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "            _ = model(input_tensor)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    avg_time = (end_time - start_time) / (num_runs * len(images))\n",
    "    return avg_time\n",
    "\n",
    "def main():\n",
    "    model_path = 'transfer_learning_models/efficientnet_terrain_classifier.pth'\n",
    "    test_dir = r\"C:\\Users\\agraw\\Downloads\\Testing Data-20241003T161706Z-001\\Testing Data\"  # Update this with the correct path to your test data\n",
    "    num_images = 10\n",
    "    num_runs = 100\n",
    "    \n",
    "    model = load_model(model_path)\n",
    "    preprocess = preprocess_image()\n",
    "    images = load_test_images(test_dir, num_images)\n",
    "    print(len(images))\n",
    "    # summary(model, (3, 224, 224))\n",
    "\n",
    "    # Test on CPU\n",
    "    cpu_time = measure_inference_time(model, images, preprocess, torch.device('cpu'), num_runs)\n",
    "    print(f\"Average CPU inference time: {cpu_time*1000:.2f} ms\")\n",
    "    \n",
    "    # Test on GPU if available\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_time = measure_inference_time(model, images, preprocess, torch.device('cuda'), num_runs)\n",
    "        print(f\"Average GPU inference time: {gpu_time*1000:.2f} ms\")\n",
    "    else:\n",
    "        print(\"CUDA is not available. GPU inference time not measured.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model File Size: 15.60 MB\n",
      "Number of Trainable Parameters: 4,013,953\n",
      "Estimated Memory Usage: 34.07 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\agraw\\AppData\\Local\\Temp\\ipykernel_24348\\4022171685.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def load_model(model_path):\n",
    "    model = models.efficientnet_b0(weights=None)\n",
    "    num_ftrs = model.classifier[1].in_features\n",
    "    model.classifier[1] = torch.nn.Linear(num_ftrs, 5)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu')))\n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def get_model_size(model_path):\n",
    "    return os.path.getsize(model_path) / (1024 * 1024)  # Size in MB\n",
    "\n",
    "def get_model_memory_usage(model, input_size=(1, 3, 224, 224)):\n",
    "    def get_tensor_size(tensor):\n",
    "        return tensor.element_size() * tensor.nelement()\n",
    "\n",
    "    total_memory = 0\n",
    "    for param in model.parameters():\n",
    "        total_memory += get_tensor_size(param)\n",
    "    \n",
    "    # Estimate memory for input, output, and intermediate activations\n",
    "    x = torch.rand(input_size)\n",
    "    model(x)  # Forward pass to initialize lazy modules\n",
    "    for module in model.modules():\n",
    "        if hasattr(module, 'weight') and hasattr(module.weight, 'data'):\n",
    "            total_memory += get_tensor_size(module.weight.data)\n",
    "        if hasattr(module, 'bias') and hasattr(module.bias, 'data'):\n",
    "            total_memory += get_tensor_size(module.bias.data)\n",
    "    \n",
    "    # Add memory for input and estimated output\n",
    "    total_memory += get_tensor_size(x)\n",
    "    total_memory += get_tensor_size(x) * 5  # Assuming 5 classes output\n",
    "\n",
    "    return total_memory / (1024 * 1024)  # Convert to MB\n",
    "\n",
    "def main():\n",
    "    model_path = 'transfer_learning_models/efficientnet_terrain_classifier.pth'\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model(model_path)\n",
    "    \n",
    "    # Get model file size\n",
    "    model_size = get_model_size(model_path)\n",
    "    \n",
    "    # Count parameters\n",
    "    num_params = count_parameters(model)\n",
    "    \n",
    "    # Estimate memory usage\n",
    "    memory_usage = get_model_memory_usage(model)\n",
    "    \n",
    "    print(f\"Model File Size: {model_size:.2f} MB\")\n",
    "    print(f\"Number of Trainable Parameters: {num_params:,}\")\n",
    "    print(f\"Estimated Memory Usage: {memory_usage:.2f} MB\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
